% 2. Códigos de Goppa.
% Esto lo puedes encontrar en el capítulo 13 de Huffman y Pless. 
% La decodificación  de estos códigos, es similar a la de los códigos BCH (capítulo 5) utilizando el algoritmo de Sugiyama. 
% El artículo donde se definen y se decodifican es el de Goppa.pdf. 
% Además, ahi se explica de forma elemental, sin utilizar geometría algebraica.

\chapter{Códigos de Goppa}

% TODO: introduccion
\textcolor{red}{Hay que completar esta introducción}

Goppa describió una nueva clase de códigos de corrección de errores lineales no cíclicos. Lo más importante es que algunos de estos códigos excedían el límite asintótico de Gilbert-Varshamov, una hazaña que muchos teóricos códigos pensaban que nunca podría lograrse. En este capítulo vamos a hablar sobre los códigos de Goppa y sus propiedades más importantes.

El desarrollo de este capítulo se ha basado en \cite{Huffman_Pless_2010} y \cite{Goppa_codes_1973}.

\section{Códigos Reed-Solomon}

Antes de estudiar los códigos de Goppa, en esta sección introduciremos los códigos Reed-Solomon, que abreviaremos como códigos RS.

\begin{definition}
    Para $k \geq 0$, $\mathcal{P}_k$ denota el conjunto de polinomios de grado menor que $k$, incluyendo el polinomio nulo, en $\mathbb{F}_q[x]$. Si $\alpha$ es la n-ésima raíz primitiva de la unidad en $\mathbb{F}_q$ donde $0 \leq k \leq n = q - 1$, entonces el código

    $$\text{RS}_k = \left\{ \left( f(1), f(\alpha), ..., f(\alpha^{q-2}) \right) : f \in \mathcal{P}_k \right\}$$

    es un $[n, k, n - k + 2]$ código Reed-Solomon sobre $\mathbb{F}_q$.
\end{definition}

\subsection{Decodificación de los códigos Reed-Solomon}

En esta sección presentaremos dos algoritmos para decodificar por el vecino más cercano los códigos Reed-Solomon. En concreto, estudiaremos los métodos de decodificación de Peterson-Gorenstein-Zierler y de Sugiyama. Este primer método consiste en cuatro pasos, en concreto el segundo de ellos destaca por ser muy complicado y llevar demasiado tiempo. Como alternativa a ejecutar este segundo paso, surge el algoritmo de Sugiyama, una aplicación simple del algoritmo de Euclides para polinomios.

\subsubsection{Algoritmo de decodificación de Peterson-Gorenstein-Zierler}

Supongamos que recibimos el mensaje $y(x)$, que difiere de la palabra código $c(x)$ en un máximo de $t$ coordenadas. El objetivo de los algoritmos de decodificación es determinar el mensaje original $c(x)$ a partir de $y(x)$. Tenemos que $y(x) = c(x) + e(x)$ donde $c(x) \in \mathcal{C}$ y $e(x)$ denota a los \emph{errores} con peso $\nu \leq t$. Supongamos que el error ocurre en las coordenadas desconocidas $k_1, ..., k_\nu$. Por lo tanto, los errores tendrán la siguiente forma

$$e(x) = e_{k_1} x^{k_1} + \cdots + e_{k_{\nu}} x^{k_{\nu}}.$$

Si determinamos $e(x)$, lo que equivale a encontrar el número de errores $\nu$, las posiciones de los errores $k_j$ y los valores de los errores $e_{k_j}$, podemos decodificar el vector recibido como $c(x) = y(x) - e(x)$.

Comenzamos definiendo el \emph{síndrome} $S_i$ de $y(x)$ como el elemento $S_i = y \left( \alpha^i \right)$ para $1 \leq i \leq 2t$. El primer paso del algoritmo será determinar los síndromes. Además, los síndromes satisfacen que

$$S_i = y (\alpha^i) = c (\alpha^i) + e (\alpha^i) = 0 + e (\alpha^i) = \sum_{j=1}^{\nu} e_{k_j} (\alpha^i) ^{k_j} = \sum_{j=1}^{\nu} e_{k_j} (\alpha^{k_j}) ^i, \qquad 1 \leq i \leq 2t.$$

Para $1 \leq j \leq \nu$ simplificaremos la notación, sea $E_j = e_{k_j}$ que denota el \emph{valor del error en la coordenada} $k_j$ y $X_j = \alpha^{k_j}$ denota el \emph{número de la posición del error correspondiente a la posición del error} $k_j$. Así, los síndromes se pueden reformular como sigue.

\begin{equation}
    \label{prop:sindromes_peterson}
    S_i = \sum_{j=1}^{\nu} E_j X_j^i, \qquad 1 \leq i \leq 2t,
\end{equation}

que resulta en el sistema de ecuaciones:

\begin{equation}
    \label{prop:sistema_sindromes_peterson}
    \left\{
    \begin{array}{c} 
        S_1 = E_1 X_1 + \cdots + E_\nu X_\nu,\\
        S_2 = E_1 X_1^2 + \cdots + E_\nu X_\nu^2\\
        \qquad \vdots \\
        S_{2t} = E_1 X_1^{2t} + \cdots + E_\nu X_\nu^{2t}
    \end{array}
    \right.
\end{equation}

Este sistema es no linear para los $X_j$ y además desconocemos los valores para $E_j$ y $X_j$. Para resolver este sistema, primero obtendremos los $X_j$ con un nuevo sistema lineal que involucrará nuevas variables. Así, el sistema \ref{prop:sistema_sindromes_peterson} será lineal para los $E_j$ y podremos determinarlos.

Para ello, definimos el \emph{polinomio localizador de errores} como

$$\sigma(x) = (1 - xX_1) \cdots (1 - xX_\nu) = 1 + \sum_{i=1}^\nu \sigma_i x^i$$

Observemos que las raíces de $\sigma(x)$ son las inversas de los $X_j$:

$$\sigma \left( X_j^{-1} \right) = 1 + \sigma_1 X_j^{-1} + \cdots + \sigma_\nu X_j^{-\nu} = 0, \qquad 1 \leq j \leq \nu .$$

Multiplicando por $E_j X_j^{i + \nu}$ obtenemos

$$E_j X_j^{i + \nu} + \sigma_1 E_j X_j^{i + \nu - 1} + \cdots + \sigma_\nu E_j X_j^{i} = 0, \qquad \forall i.$$

Si sumamos para todo $j$ en $1 \leq j \leq \nu$ tenemos

$$\sum_{j=1}^\nu E_j X_j^{i + \nu} + \sigma_1 \sum_{j=1}^\nu E_j X_j^{i + \nu - 1} + \cdots + \sigma_\nu \sum_{j=1}^\nu E_j X_j^{i} = 0.$$

En estas sumas hemos obtenido los síndromes de \ref{prop:sindromes_peterson}, pues $i \geq 1$ y $i + \nu \leq 2t$. Como $\nu \leq t$, tenemos que

$$S_{i + \nu} + \sigma_1 S_{i + \nu - 1} + \cdots + \sigma_\nu S_i = 0, \qquad 1 \leq i \leq \nu ,$$

que equivale a 

$$\sigma_1 S_{i + \nu - 1} + \cdots + \sigma_\nu S_i = -S_{i + \nu}, \qquad 1 \leq i \leq \nu .$$

Así, llegamos al siguiente sistemas de ecuaciones del que podemos obtener los $\sigma_k$.

\begin{equation}
    \label{prop:sistema_polinomio_localizador_errores}
    \left(
        \begin{array}{ccccc}
            S_1 & S_2 & \cdots & S_{\nu - 1} & S_\nu \\
            S_2 & S_3 & \cdots & S_\nu & S_{\nu + 1} \\
                &     & \vdots &       &             \\
            S_\nu & S_{\nu + 1} & \cdots & S_{2\nu - 2} & S_{2\nu - 1}
        \end{array}
    \right)
    \left(
        \begin{array}{c}
            \sigma_\nu \\
            \sigma_{\nu - 1}\\ 
            \vdots \\
            \sigma_{1}
        \end{array}
    \right)
    = 
    \left(
        \begin{array}{c}
            -S_{\nu + 1} \\
            -S_{\nu + 2}\\ 
            \vdots \\
            -S_{2\nu}
        \end{array}
    \right)
\end{equation}

La dificultad para resolver este sistema está en que desconocemos el valor de $\nu$, que queremos que sea lo más pequeño posible. El siguiente lema nos será útil.

\begin{lemma}
    \label{lm:matriz_singular}
    Sea $\mu \leq t$ y sea 
    \[
        \left(
        \begin{array}{ccc}
            S_1 & \cdots & S_\mu \\
            \vdots & \ddots & \vdots \\
            S_\mu & \cdots & S_{2\mu - 1}
        \end{array}
        \right)
    \]
    Si $\mu = \nu$, entonces $M_\mu$ no es singular; pero si $\mu > \nu$, entonces $M_\mu$ es singular, donde $\nu$ es el número de errores que se han producido.
\end{lemma}

\begin{proof}
    Ver Lema 5.4.2 en la página 181, \cite{Huffman_Pless_2010}.
\end{proof}

Realizaremos un procedimiento iterativo para obtener el valor de $\nu$, empezando con $\mu = t$, que es lo más grande que $\nu$ puede ser. Teniendo en cuenta el lema \ref{lm:matriz_singular}, tenemos que $M_\mu = M_t$. Luego si $M_\mu$ es singular, reducimos el valor de $\mu$ en $1$, $\mu = t - 1$, y volvemos a probar si $M_\mu = M_{t-1}$ es singular. Continuaremos reduciendo $\mu$ en $1$ hasta que obtengamos una matriz $M_\mu$ que no sea singular, en cuyo caso obtendremos el valor de $\nu = \mu$. Así, resolvemos el sistema \ref{prop:sistema_polinomio_localizador_errores} y podremos determinar $\sigma(x)$. Con esto concluimos el segundo paso del algoritmo.

El siguiente paso será determinar las raíces de $\sigma(x)$ y calcular sus inversas para determinar los $X_j$. Esto lo podemos hacer calculando reiteradamente $\sigma(\alpha^i)$ para $0 \leq i < n$. Una vez conocidos los $X_j$, podemos resolver el sistema de ecuaciones \ref{prop:sistema_sindromes_peterson} para determinar los $E_j$. Ahora ya podremos determinar los valores de $k_j$ y $e_{k_j}$, y así poder obtener el vector error $e(x)$. Finalmente, restando el vector $e(x)$ al vector $y(x)$ podemos determinar el mensaje original $c(x)$, como queríamos.

En resumen, el algoritmo de decodificación de Peterson-Gorenstein-Zierler es el siguiente:

\begin{itemize}
    \item[I.] Calcular los síndromes $S_i = y(\alpha^i)$ para $1 \leq i \leq 2t$.
    \item[II.] Iterar decrementando una unidad desde $\mu = t$ hasta que $M_{\mu}$ no sea singular. Definamos ese valor como $\nu = \mu$ y resolvemos \ref{prop:sistema_polinomio_localizador_errores} para obtener $\sigma(x)$.
    \item[III.] Determinar las raíces de $\sigma(x)$ calculando $\sigma(\alpha^i)$ para $0 \leq i < n$. Los $X_j$ coinciden con las inversas de dichas raíces.
    \item[IV.] Resolver las primeras $\nu$ ecuaciones de \ref{prop:sistema_sindromes_peterson} para determinar los $E_j$.
    \item[V.] Calcular los valores de $k_j$ y $e_{k_j}$.
    \item[VI.] Determinar $c(x)$ como el resultado de la operación $y(x) - e(x)$.
\end{itemize}

\begin{exampleth}
    % TODO
    \textcolor{red}{Poner un ejemplo chorra}
\end{exampleth}

\subsubsection{Algoritmo de decodificación de Sugiyama}

El algoritmo de decodificación de Peterson-Gorenstein-Zierler se puede mejorar, en concreto el paso II consume mucho tiempo pues tiene que computar diversos determinantes para cada matriz hasta encontrar alguna que no sea singular. El algoritmo de Sugiyama presenta una alternativa para este segundo paso, de hecho usa el algoritmo de Euclides para determinar el polinomio localizador de errores de una manera más eficiente.

Recordamos que el polinomio localizador de errores $\sigma(x)$ se define como $\prod_{j=1}^{\nu} (1 - xX_j)$. El \emph{polinomio evaluador de errores} $\omega(x)$ se define como

\begin{equation}
    \label{def:polinomio_evaluador_errores}
    \omega(x) = \sum_{j=1}^{\nu} E_j X_j \prod_{\substack{i=1\\ i \neq j}} (1 - xX_i) = \sum_{j=1}^{\nu} E_j X_j \frac{\sigma(x)}{1 - xX_j}.
\end{equation}

Observemos que $gr(\sigma(x)) = \nu$ y $gr(\omega(x)) \leq \nu - 1$. Definimos el polinomio $S(x)$ de grado como mucho $2t - 1$ tal que 

$$S(x) = \sum_{i=0}^{2t - 1} S_{i+1} x^i,$$

donde $S_i$ para $1 \leq i \leq 2t$ son los síndromes.

De esta forma, se cumple la siguiente relación (página 191, \cite{Huffman_Pless_2010}.)

$$\omega(x) \equiv \sigma(x) S(x) \pmod{x^{2t}},$$

a la que nos referiremos como $ecuación clave$. Además, tenemos que los polinomios $\sigma(x)$ y $\omega(x)$ son primos relativos.

El algoritmo de Sugiyama es el siguiente.

\begin{itemize}
    \item[I.] Sean $f(x) = x^{2t}$, $s(x) = S(x)$, $r_{-1}(x) = f(x)$, $r_0(x) = s(x)$, $b_{-1}(x) = 0$ y $b_0(x) = 1$.
    \item[II.] Iterar incrementando en una unidad desde $i = 1$ hasta $I$, de forma que se cumpla que $gr(r_{I-1}(x)) \geq t$ y $gr(r_I(x)) < t$. En cada iteración habrá que determinar $h_i(x)$, $r_i(x)$ y $b_i(x)$:
        $$r_{i-2}(x) = r_{i-1}(x) h_i(x) + r_i (x), \qquad \text{ donde } gr(r_i(x)) < gr(r_{i-1}(x)),$$
        $$b_i(x) = b_{i-2}(x) - h_i(x) b_{i-1}(x).$$
    \item[III.] $\sigma(x)$ es un escalar no nulo múltiplo de $b_I(x)$. 
\end{itemize}

Para verificar que este algoritmo funciona, el siguiente lema nos será de utilidad.

\begin{lemma}
    \label{lm:dem_sugiyama}
    Con la notación del algoritmo de Sugiyama, sea $a_{-1}(x) = 1$, $a_0(x) = 0$ y $a_i(x) = a_{i-2}(x) - h_i(x) a_{i-1}(x)$ para $i \geq 1$. Las siguientes afirmaciones son ciertas.

    \begin{itemize}
        \item $a_i(x) f(x) + b_i(x) s(x) = r_i (x)$ para $i \geq -1$.
        \item $b_i(x) r_{i-1}(x) - b_{i-1} r_i(x) = (-1)^i f(x)$ para $i \geq 0$.
        \item $a_i(x) b_{i-1}(x) - a_{i-1} b_i (x) = (-1)^{i+1}$ para $i \geq 0$.
        \item $gr(b_i(x)) + gr(r_{i-1}(x)) = gr(f(x))$ para $i \geq 0$.
    \end{itemize}
\end{lemma}

\begin{proof}
    Ver Lema 5.4.11 en la página 191, \cite{Huffman_Pless_2010}.
\end{proof}

Comprobemos ahora que el algoritmo de Sugiyama funciona. Por el Lema \ref{lm:dem_sugiyama} (i) tenemos que

\begin{equation}
    \label{dem:sugiyama_relacion}
    a_I(x) x^{2t} + b_I(x) S(x) = r_I(x).
\end{equation}

Además, por la ecuación clave sabemos que

\begin{equation}
    \label{dem:sugiyama_key_equation}
    a(x) x^{2t} + \sigma(x) S(x) = \omega(x)
\end{equation}

para algún polinomio $a(x)$. Multiplicando \ref{dem:sugiyama_relacion} por $\sigma(x)$ y \ref{dem:sugiyama_key_equation} por $b_I(x)$ obtenemos 

\begin{equation}
    \label{dem:sugiyama_relacion_sigma}
    a_I(x) \sigma(x) x^{2t}  + b_I(x) \sigma(x) S(x) = r_I(x) \sigma(x) \quad \text{ y }
\end{equation}
\begin{equation}
    \label{dem:sugiyama_key_equation_b_I}
    a(x) b_I(x) x^{2t} + \sigma(x) b_I(x) S(x) = \omega(x) b_I(x).
\end{equation}

Aplicando módulo $x^{2t}$ a ambas ecuaciones tenemos que 

\begin{align*}
    b_I(x) \sigma(x) S(x) &\equiv r_I(x) \sigma(x) \pmod{x^{2t}} \quad \text{ y }\\
    \sigma(x) b_I(x) S(x) &\equiv \omega(x) b_I(x) \pmod{x^{2t}}.
\end{align*}

Por lo tanto,

\begin{equation}
    \label{dem:sugiyama_modulo}
    r_I(x) \sigma(x) \equiv \omega(x) b_I(x) \pmod{x^{2t}}.
\end{equation}

Como $gr(\sigma(x)) \leq t$, por la elección de $I$,

$$gr(r_I(x) \sigma(x)) = gr(r_I(x)) + gr(\sigma(x)) < t + t = 2t.$$ 

Por el Lema \ref{lm:dem_sugiyama} (iv), la elección de $I$ y como $gr(\omega(x)) < t$, entonces se cumple que 

$$gr(\omega(x) \times b_I(x)) = gr(\omega(x)) + gr(b_I(x)) < t + gr(b_I(x)) = t + (gr(x^{2t}) - gr(r_{I-1}(x))) \leq 3t - t = 2t.$$

Por \ref{dem:sugiyama_modulo} tenemos que $r_I(x) \sigma(x) = \omega(x) b_I(x)$. Esto, junto con \ref{dem:sugiyama_relacion_sigma} y \ref{dem:sugiyama_key_equation_b_I}, implica que 

\begin{equation}
    \label{dem:sugiyama_implicacion_ecuaciones}
    r_I(x) \sigma(x) \equiv \omega(x) b_I(x) \pmod{x^{2t}}.
\end{equation}

Sin embargo, el Lema \ref{lm:dem_sugiyama} (iii) afirma que $a_I(x)$ y $b_I(x)$ son primos relativos, y por \ref{dem:sugiyama_implicacion_ecuaciones} se cumple que $a(x) = \lambda(x) a_I(x)$. Sustituyendo esta relación en \ref{dem:sugiyama_implicacion_ecuaciones},

\begin{equation}
    \label{dem:sugiyama_sustitucion}
    \sigma(x) = \lambda(x) b_I(x).
\end{equation}

Ahora, sustituyendo estas dos últimas relaciones en \ref{dem:sugiyama_key_equation} obtenemos $\lambda(x) a_I(x) x^{2t} + \lambda(x) b_I(x) S(x) = \omega(x)$. La ecuación \ref{dem:sugiyama_relacion} implica que

\begin{equation}
    \label{dem:sugiyama_relacion_omega}
    \omega(x) = \lambda(x) a_I(x) x^{2t} + \lambda(x) b_I(x) S(x) = \lambda(x) \cdot \left( a_I(x) x^{2t} + b_I(x) S(x) \right) = \lambda(x) r_I(x).
\end{equation}

Teniendo en cuenta que los polinomios $\sigma(x)$ y $\omega(x)$ son primos relativos y por \ref{dem:sugiyama_sustitucion} y \ref{dem:sugiyama_relacion_omega}, $\lambda(x)$ tiene que ser una constante no nula, verificando el paso III del algoritmo de Sugiyama.

Como solo estamos interesados en las raíces de $\sigma(x)$, es suficiente con determinar las raíces de $b_I(x)$ obtenidos en el paso II; lo que nos dará los $X_j$. 

Más adelante veremos que este algoritmo sigue funcionando con otras elecciones de $f(x)$ y $s(x)$, con las modificaciones apropiadas de las condiciones sobre las que el algoritmo termina en el paso II. Una de estas modificaciones nos será útil para decodificar los códigos Goppa que veremos a continuación.

\section{Códigos clásicos de Goppa}

Los códigos clásicos de Goppa se introdujeron por V. D. Goppa en 1970. Estos códigos son generalizaciones de códigos BCH y subcódigos de subcuerpos de ciertos códigos GRS.

Para motivar la definición de los códigos de Goppa, introduciremos una construcción de los códigos BCH de longitud $n$ sobre $\mathbb{F}_q$. Sea $t = ord_q(n)$ y sea $\beta$ la raíz enésima primitiva de la unidad en $\mathbb{F}_{q^t}$. Elegimos $\delta > 1$ y sea $\mathcal{C}$ el código BCH de longitud $n$ y distancia $\delta$. Entonces $c(x) = c_0 + c_1x + \cdots + c_{n-1}x^{n-1} \in \mathbb{F}_q [x] / (x^n - 1)$ está en $\mathcal{C}$ si y solo si $c(\beta^j) = 0$ para $1 \leq j \leq \delta - 1$. Tenemos que 

$$(x^n - 1) \sum_{i=0}^{n-1} \frac{c_i}{x - \beta ^{-i}} = \sum_{i=0}^{n-1} c_i \sum_{l=0}^{n-1} x^l \left( \beta ^{-i} \right) ^{n-1-l} = \sum_{l=0}^{n-1} x^l \sum_{i=0}^{n-1} c_i \left( \beta^{l+1} \right) ^i.$$

Como $c(\beta^{l+1}) = 0$ para $0 \leq l \leq \delta - 2$, el lado derecho de la ecuación es un polinomio cuyo término de menor grado tiene grado al menos $\delta - 1$. Por lo tanto, el lado derecho se puede escribir como $x^{\delta - 1} p(x)$, donde $p(x)$ es un polinomio en $\mathbb{F}_{q^t}[x]$. Así, podemos decir que $c(x) \in \mathbb{F}_q[x] / (x^n - 1)$ está en $\mathcal{C}$ si y solo si 

$$\sum_{i=0}^{n-1} \frac{c_i}{x - \beta ^{-i}} = \frac{x^{\delta - 1} p(x)}{x^n - 1}$$

o equivalentemente

$$\sum_{i=0}^{n-1} \frac{c_i}{x - \beta ^{-i}} \equiv 0 \pmod{x^{\delta - 1}}$$

La última equivalencia es la base para la definición de los códigos clásicos de Goppa.

Fijado el cuerpo de extensión $\mathbb{F}_{q^t}$ de $\mathbb{F}_q$, sea $L = \{ \gamma_0, ..., \gamma_{n-1} \}$ una tupla de $n$ elementos distintos de $\mathbb{F}_{q^t}$ y sea $g(x) \in \mathbb{F}_{q^t}[x]$ con $g(\gamma_i) \neq 0$ para $0 \leq i \leq n - 1$. Entonces el \emph{código de Goppa} $\Gamma(L,g)$ es el conjunto de vectores $c_0 \cdots c_{n-1} \in \mathbb{F}_q^n$ tal que 

\begin{equation}
    \label{def:goppa}
    \sum_{i=0}^{n-1} \frac{c_i}{x - \gamma_i} \equiv 0 \pmod{g(x)}
\end{equation}

De esta forma, cuando la parte de la izquierda está escrita como una función racional, significa que el numerador es un múltiplo de $g(x)$. Además, trabajar con módulo $g(x)$ es como trabajar en el anillo $\mathbb{F}_{q^t}[x]/(g(x))$, y la hipótesis $g(\gamma_i) \neq 0$ garantiza que $x - \gamma_i$ es invertible en este anillo. Se llama a $g(x)$ el \emph{polinomio de Goppa} de $\Gamma(L,g)$. Notemos que el código BCH de longitud $n$ y la distancia elegida $\delta$ es el código de Goppa $\Gamma(L,g)$ con $L = \{ 1, \beta^{-1}, ..., \beta^{1-n} \}$ y $g(x) = x^{\delta - 1}$.

A continuación, buscaremos una matriz de paridad para $\Gamma(L,g)$. Para ello, observamos que

$$\frac{1}{x - \gamma_i} \equiv - \frac{1}{g(\gamma_i)} \frac{g(x) - g(\gamma_i)}{x - \gamma_i} \pmod{ g(x)}$$

ya que, comparando numeradores, $1 \equiv - g(\gamma_i)^{-1} \left( g(x) - g(\gamma_i) \right) \pmod{g(x)}$. Así que por \eqref{def:goppa} $\textbf{c} = c_0 \cdots c_{n-1} \in \Gamma(L,g)$ si y solo si

\begin{equation}
    \label{congruencia_goppa}
    \sum_{i=0}^{n-1} c_i \frac{g(x) - g(\gamma_i)}{x - \gamma_i} g(\gamma_i)^{-1} \equiv 0 \pmod{g(x)}
\end{equation}

Supongamos que $g(x) = \sum_{j=0}^w g_j x^j$ con $g_j \in \mathbb{F}_{q^t}$, donde $w = gr(g(x))$. Entonces

$$\frac{g(x) - g(\gamma_i)}{x - \gamma_i} g(\gamma_i)^{-1} = g(\gamma_i)^{-1} \sum_{j=1}^w g_j \sum_{k=0}^{j-1} x^k \gamma_i^{j-1-k} = g(\gamma_i)^{-1} \sum_{k=0}^{w-1} x^k \left( \sum_{j=k+1}^w g_j \gamma_i^{j-1-k} \right)$$

Por lo tanto, por \eqref{congruencia_goppa}, estableciendo los coeficientes de $x^k$ iguales a $0$, en el orden $k = w - 1, w - 2, ..., 0$, tenemos que $\textbf{c} \in \Gamma(L,g)$ si y solo si $Hc^T = 0$, donde 

\begin{equation}
    H = \left(
        \begin{array}{ccc} 
            h_0 g_w & \cdots & h_{n-1} g_w  \\
            h_0 (g_{w-1} + g_w \gamma_0) & \cdots & h_{n-1} (g_{w-1} + g_w \gamma_{n-1}) \\
            & \vdots & \\
            h_0 \sum_{j=1}^w \left( g_j + \gamma_0^{j-1} \right) & \cdots & h_{n-1} \sum_{j=1}^w \left( g_{j} + \gamma_{n-1}^{j-1} \right) \\
        \end{array}
        \right)
\end{equation}

con $h_i = g(\gamma_i)^{-1}$.

\begin{proposition}
    La matriz $H$ se puede reducir a una matriz $H'$ de dimensión $w \times n$, donde 

    \begin{equation}
        H' = \left(
            \begin{array}{ccc} 
                g(\gamma_0)^{-1} & \cdots & g(\gamma_{n-1})^{-1}  \\
                g(\gamma_0)^{-1} \gamma_0 & \cdots & g(\gamma_{n-1})^{-1} \gamma_{n-1} \\
                & \vdots & \\
                g(\gamma_0)^{-1} \gamma_0^{w-1} & \cdots & g(\gamma_{n-1})^{-1} \gamma_{n-1}^{w-1} \\
            \end{array}
            \right)
    \end{equation}
\end{proposition}

Las entradas de $H'$ están en $\mathbb{F}_{q^t}$. Eligiendo una base de $\mathbb{F}_{q^t}$ sobre $\mathbb{F}_q$, cada elemento de $\mathbb{F}_{q^t}$ se puede representar como un vector columna $t \times 1$ sobre $\mathbb{F}_q$. Reemplazando cada entrada de $H'$ por su correspondiente vector columna, obtenemos una matriz $H''$ de dimensión $tw \times n$ sobre $\mathbb{F}_{q}$ que tiene la propiedad de que si $\textbf{c} \in \mathbb{F}_q^n$ está en $\Gamma(L,g)$ si y solo si $H''c^T = 0$.

El siguiente resultado nos muestra los límites en la dimensión y la distancia mínima de un código de Goppa.

\begin{theorem}
    \label{th:dist_min_Goppa}
    Con la notación de esta sección, sea $\Gamma(L,g)$ un código de Goppa tal que $gr(g(x)) = w$ entonces es un $[n, k, d]$ código con $k \geq n - wt$ y $d \geq w + 1$.
\end{theorem}

\begin{proof}
    Las filas de $H''$ pueden ser dependientes, luego esta matriz tiene rango como máximo $wt$. Por lo que $\Gamma(L,g)$ tiene dimensión al menos $n - wt$. Si una palabra código $\textbf{c} \in \Gamma(L,g)$ tiene peso $w$ o menos, entonces el lado izquierdo de \ref{def:goppa} es una función racional, donde el numerador tiene grado $w - 1$ o menos; pero este numerador tiene que ser múltiplo de $g(x)$, lo cual es una contradicción pues el grado de $g$ es $w$.
\end{proof}

\begin{corollary}
    Si $\Gamma(L,g)$ es un código de Goppa tal que $gr(g(x)) = w$, entonces puede corregir hasta

    $$\left\lfloor \frac{w}{2} \right\rfloor $$

    errores.
\end{corollary}

\begin{proof}
    El teorema \ref{th:decodificacion_maxima_verosimilitud} afirma que es posible corregir hasta 
    
    $$\left\lfloor \frac{d(\Gamma(L,g)) - 1}{2} \right\rfloor$$

    errores. Además, por el teorema \ref{th:dist_min_Goppa} sabemos que la distancia mínima de un código de Goppa $\Gamma(L,g)$ tal que $gr(g(x)) = w$ es $d(\Gamma(L,g)) = w + 1$. Por lo que

    $$\left\lfloor \frac{d(\Gamma(L,g)) - 1}{2} \right\rfloor = \left\lfloor \frac{(w + 1) - 1}{2} \right\rfloor = \left\lfloor \frac{w}{2} \right\rfloor .$$
\end{proof}

\subsection{Códigos binarios de Goppa}

Los códigos binarios de Goppa son códigos de corrección de errores que pertenecen a la clase de los códigos de Goppa que acabamos de estudiar. La estructura binaria le da más ventajas matemáticas sobre variantes no binarias y, además, tienen propiedades interesantes adecuadas para la construcción del criptosistema McEliece.

\begin{definition}

    Fijado el cuerpo de extensión $\mathbb{F}_{2^m}$ de $\mathbb{F}_2$, sea $L = \{ \gamma_0, ..., \gamma_{n-1} \} \in \mathbb{F}_{2^m}^n$ una tupla de $n$ elementos distintos de $\mathbb{F}_{2^m}$ y sea $g(x) \in \mathbb{F}_{2^m}[x]$ con $g(\gamma_i) \neq 0$ para $0 \leq i \leq n - 1$. Entonces el \emph{código binario de Goppa} $\Gamma(L,g)$ es el conjunto de vectores $c_0 \cdots c_{n-1} \in \{ 0, 1 \}^n$ tal que 

    \begin{equation}
        \sum_{i=0}^{n-1} \frac{c_i}{x - \gamma_i} \equiv 0 \pmod{g(x)}
    \end{equation}
\end{definition}

Observemos que si $g(x)$ es un polinomio irreducible, todos los elementos $\gamma \in \mathbb{F}_{2^m}$ satisfacen $g(\gamma) \neq 0$. A los códigos que cumplan esta propiedad los llamaremos \emph{códigos binarios de Goppa irreducibles}.

En los siguientes resultados se enuncian las propiedades de los códigos binarios de Goppa, que tienen ciertas diferencias con los generales.

\begin{theorem}
    Sea $\Gamma(L,g)$ un código binario de Goppa tal que $gr(g(x)) = w$ entonces es un $[n, k, d]$ código con $k \geq n - wt$ y $d \geq 2w + 1$.
\end{theorem}

\begin{corollary}
    Si $\Gamma(L,g)$ es un código de Goppa tal que $gr(g(x)) = w$, entonces puede corregir hasta

    $$\left\lfloor \frac{(2w + 1) - 1}{2} \right\rfloor $$

    errores.
\end{corollary}

Observamos que con estos códigos podemos doblar la capacidad correctora de los códigos generales de Goppa.


\subsection{Decodificación de los códigos de Goppa}

Como hemos visto, al transmitir una palabra código a un receptor, éste podría recibir la palabra alterada. Para que el receptor pueda determinar el mensaje original, necesitaremos decodificar el mensaje recibido. Supongamos que $E$ es el vector de errores que se añade a la palabra código $C$ transmitida, entonces la palabra recibida $R$ está dada por

$$R = C + E$$

de donde 

$$\sum_{\gamma \in L} \frac{R_\gamma}{x - \gamma} = \sum_{\gamma \in L} \frac{C_\gamma}{x - \gamma} + \sum_{\gamma \in L} \frac{E_\gamma}{x - \gamma}.$$

Como $C$ es una palabra código, la primera sumatoria de la parte derecha desaparece al aplicar el módulo $g(x)$, y tenemos que

$$\sum_{\gamma \in L} \frac{R_\gamma}{x - \gamma} = \sum_{\gamma \in L} \frac{E_\gamma}{x - \gamma} \pmod{g(x)}.$$

Definiremos su síndrome como el polinomio $S(x)$ de grado menos que $gr(g(x))$ tal que 

$$S(x) = \sum_{\gamma \in L} \frac{R_\gamma}{x - \gamma} \pmod{g(x)}.$$

Acabamos de ver que 

$$S(x) = \sum_{\gamma \in L} \frac{E_\gamma}{x - \gamma} \pmod{g(x)}.$$

Sea $M$ un subconjunto de $L$ tal que $E_{\gamma} \neq 0$ si y solo si $\gamma \in M$. Entonces

\begin{equation}
    \label{def:sindrome}
    S(x) = \sum_{\gamma \in M} \frac{E_\gamma}{x - \gamma} \pmod{g(x)}.
\end{equation}

De esta forma, ahora podemos introducir el polinomio cuyas raíces son las ubicaciones de los errores,

\begin{equation}
    \label{def:localizaciones}
    \sigma (x) = \prod_{\gamma \in M} (x - \gamma)
\end{equation}

Sin embargo, para los códigos de Goppa es más conveniente definir una variante de este polinomio de la siguiente forma.

\begin{equation}
    \label{def:eta}
    \eta (x) = \sum_{\gamma \in M} E_\gamma \prod_{\partial \in M \setminus \{ \gamma \} } (x - \partial)
\end{equation}

Observemos que de esta forma $\sigma(x)$ y $\eta(x)$ deben ser primos relativos.

Derivando la expresión de $\sigma(x)$, tenemos que 

\begin{equation}
    \label{def:localizaciones_derivada}
    \sigma'(x) = \sum_{\gamma \in M} \prod_{\partial \in M \setminus \{ \gamma \} } (x - \partial)
\end{equation}


de donde, para cada $\gamma \in M$,

$$\eta (\gamma) = E_\gamma \prod_{\partial \in M \setminus \{ \gamma \} } (\gamma - \partial) = E_\gamma \sigma'(\gamma)$$

por lo que $E_\gamma = \frac{\eta(\gamma)}{\sigma'(\gamma)}$. De esta forma, una vez que hemos calculado los polinomios $\sigma$ y $\eta$, las coordenadas del vector error vienen dadas por 

\[
    E_\gamma = \left\{ \begin{array}{lcc}
    0 &   \text{si}  & \sigma(\gamma) \neq 0 \\
    \\ \frac{\eta(\gamma)}{\sigma'(\gamma)} &  \text{si} & \sigma(\gamma) = 0
    \end{array}
    \right.
\]

donde $\sigma'(x)$ es la derivada de $\sigma(x)$.

Lo esencial para decodificar los códigos de Goppa es determinar los coeficientes de los polinomios $\sigma$ y $\eta$. Para ello, tenemos que relacionar $\sigma$ y $\eta$ al síndrome de la ecuación \ref{def:sindrome}. Esto se consigue multiplicando las ecuaciones \ref{def:sindrome} y \ref{def:localizaciones}, obteniendo

\begin{equation}
    \label{prop:key_equation}
    S(x) \cdot \sigma(x) \equiv \eta(x) \pmod{g(x)}
\end{equation}

La ecuación \ref{prop:key_equation} es la \emph{ecuación clave} para decodificar los códigos de Goppa. Dado $g(x)$ y $S(x)$, el problema de decodificar consiste en encontrar polinomios de grado bajo $\sigma(x)$ y $\eta(x)$ que satisfacen \ref{prop:key_equation}.

Reduciendo cada potencia de $x \pmod{g(x)}$ e igualando coeficientes de $1, x, ..., x^{gr g - 1}$, tenemos que \ref{prop:key_equation} es un sistema de $gr G$ ecuaciones lineales donde las incógnitas son los coeficientes de $\sigma$ y $\eta$. Por lo tanto, para probar que el decodificador es capaz de corregir todos los patrones hasta $t$ errores, basta con probar que \ref{prop:key_equation} tiene una única solución con grados de $\sigma$ y de $\eta$ suficientemente pequeños. Esto equivale a que el conjunto de ecuaciones lineales correspondientes sean linealmente independientes.

Supongamos que existen dos pares diferentes de soluciones a \ref{prop:key_equation}:

\begin{equation}
    \label{prop:key_equation_1}
    S(x) \sigma^{(1)}(x) \equiv \eta^{(1)}(x) \pmod{g(x)}
\end{equation}

\begin{equation}
    \label{prop:key_equation_2}
    S(x) \sigma^{(2)}(x) \equiv \eta^{(2)}(x) \pmod{g(x)}
\end{equation}

donde $\sigma^{(1)}(x)$ y $\eta^{(1)}(x)$ son primos relativos, al igual que $\sigma^{(2)}(x)$ y $\eta^{(2)}(x)$. Además, $\sigma^{(1)}(x)$ y $g(x)$ no pueden tener ningún factor en común, pues en ese caso ese factor podría dividir a $\eta^{(1)}(x)$, contradiciendo que $\sigma^{(1)}(x)$ y $\eta^{(1)}(x)$ son primos relativos. Así, podemos dividir \ref{prop:key_equation_1} por $\sigma^{(1)}(x)$ y obtenemos

$$S(x) \equiv \frac{\eta^{(1)}(x)}{\sigma^{(1)}(x)} \pmod{g(x)}$$

de la misma forma para \ref{prop:key_equation_2},

$$S(x) \equiv \frac{\eta^{(2)}(x)}{\sigma^{(2)}(x)} \pmod{g(x)}$$

de donde,

\begin{equation}
    \label{prop:key_equation_12}
    \sigma^{(1)}(x) \eta^{(2)}(x) \equiv \sigma^{(2)}(x) \eta^{(1)}(x) \pmod{g(x)}
\end{equation}

Si $gr(G) = 2t$ y $gr(\sigma^{(1)}) \leq t$, $gr(\sigma^{(2)}) \leq t$, $gr(\eta^{(2)}) < t$ y $gr(\eta^{(1)}) < t$, entonces se da la siguiente igualdad

\begin{equation}
    \label{prop:key_equation_caso1}
    \sigma^{(1)}(x) \eta^{(2)}(x) = \sigma^{(2)}(x) \eta^{(1)}(x)
\end{equation}

Así, $\sigma^{(1)}$ divide a $\sigma^{(2)} \eta^{(1)}$, y como $\sigma^{(1)}$ y $\eta^{(1)}$ son primos relativos, $\sigma^{(1)}$ tiene que dividir a $\sigma^{(2)}$. Análogamente, $\sigma^{(2)}$ tiene que dividir a $\sigma^{(1)}$. Como ambos son mónicos, se tiene que $\sigma^{(1)} = \sigma^{(2)}$ y así, $\eta^{(1)} = \eta^{(2)}$. Con esto hemos probado que si el grado de $G$ es $2t$, entonces \ref{prop:key_equation} tiene una única solución cuando $gr(\eta) < gr(\sigma) \leq t$, luego el correspondiente sistema de ecuaciones lineales donde las incógnitas son los coeficientes de $\sigma$ y $\eta$ tiene que ser no singular. En el siguiente teorema se concluye este resultado.

\begin{theorem}
    Si $gr(g(x)) = 2t$, entonces hay un algoritmo de decodificación algebraica de corrección de $t$ errores para el código q-ario de Goppa con el polinomio de Goppa $g(x)$.
\end{theorem}

Estudiemos ahora este resultado en el caso binario, primero observamos que ya que todos los $E_\gamma$ distintos de cero son iguales a 1, entonces \ref{def:eta} y \ref{def:localizaciones_derivada} coincidan. De esta forma, la ecuación \ref{prop:key_equation_12} ahora pasa a ser

$$\sigma^{(1)} \left( \sigma^{(2)} \right) ' \equiv \sigma^{(2)} \left( \sigma^{(1)} \right) ' \pmod{g(x)}$$

Ahora, cuando $\sigma$ sea par escribiremos en su lugar $\hat{\sigma}$, mientras que cuando $\sigma$ sea impar escribiremos en su lugar $x \sigma '$. Así, tenemos que

\begin{align*} 
    \left( \hat{\sigma}^{(1)} + x \sigma^{(1)'} \right) \sigma^{(2)'} &\equiv \left( \hat{\sigma}^{(2)} + x \sigma^{(2)'} \right) \sigma^{(1)'}\\ 
    \hat{\sigma}^{(1)} \sigma^{(2)'} + \hat{\sigma}^{(2)} \sigma^{(1)'} &\equiv 0 \pmod{g(x)}.
\end{align*}

El lado izquierdo es un cuadrado perfecto, pues todos los polinomios de ese lado son pares. Esto implica que

$$\hat{\sigma}^{(1)} \sigma^{(2)'} + \hat{\sigma}^{(2)} \sigma^{(1)'} \equiv 0 \pmod{\bar{G}(x)}$$

donde $\bar{G}(x)$ es múltiplo de $g(x)$ de menor grado ya que $\bar{G}$ es un cuadrado perfecto. Por lo que, si $gr(\bar{G}) = 2t$, $gr(\sigma^{(1)}) \leq t$ y $gr(\sigma^{(2)}) \leq t$, entonces 

$$\hat{\sigma}^{(1)} \left( \sigma^{(2)} \right) ' = \hat{\sigma}^{(2)} \sigma^{(1)'}.$$

Por la primalidad relativa, $\sigma^{(1)} = \sigma^{(2)}$. En el siguiente teorema se concluye este resultado.

\begin{theorem}
    Si $gr(g(x)) = t$ y si $g(x)$ no tiene factores irreducibles repetidos, entonces hay un algoritmo de decodificación algebraica de corrección de $t$ errores para el código binario de Goppa con el polinomio de Goppa $g(x)$.
\end{theorem}

\subsubsection{El algoritmo de decodificación de Sugiyama}

En esta sección presentaremos una modificación del algoritmo de Sugiyama, ya estudiado para los códigos RS, para adaptarlo a los códigos Goppa.

Dado $g(x)$ un polinomio de Goppa con grado $2t$, el algoritmo de decodificación de Sugiyama para los códigos Goppa es el siguiente.

\begin{itemize}
    \item[I.] Calcular el síndrome $S(x)$.
    \item[II.] Sean $r_{-1}(x) = g(x)$, $r_0(x) = S(x)$, $U_{-1}(x) = 0$ y $U_0(x) = 1$.
    \item[III.] Buscar $q_i(x)$ y $r_i(x)$ aplicando el algoritmo de Euclides para encontrar el máximo común divisor de $r_{-1}(x)$ y $r_0(x)$ para $i = 1,..., k$, hasta que $k$ cumpla que $gr(r_{k-1}(x)) \geq t$ y $gr(r_k(x)) < t$:

        $$r_{i-2}(x) = r_{i-1}(x) q_i(x) + r_i(x), \qquad gr(r_i(x)) < gr(r_{i-1})(x)$$
    
    \item[IV.] Calcular $U_k(x)$, donde
    
        $$U_i(x) = q_i(x) U_{i-1}(x) + U_{i-2}(x)$$

    \item[V.] La solución viene dada por:
        $$\eta(x) = (-1)^k \delta r_k(x)$$
        $$\sigma(x) = \delta U_k(x)$$
\end{itemize}

\begin{proposition}
    Sea $e$ el número de errores que ocurren realmente y sea $k$ el número de iteraciones del algoritmo descrito, entonces $k \leq e$.
\end{proposition}